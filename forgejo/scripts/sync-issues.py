#!/usr/bin/env -S uv run
# /// script
# dependencies = [
#   "requests>=2.31.0",
# ]
# requires-python = ">=3.9"
# ///

import argparse
import json
import os
import sys
import subprocess
from pathlib import Path
from datetime import datetime, timezone
import requests

# Add path to core scripts for sync_manager and id_registry
core_scripts_path = Path(__file__).parent.parent.parent / "core" / "scripts"
sys.path.insert(0, str(core_scripts_path))
from sync_manager import update_sync_metadata
from id_registry import assign_id, lookup_id

CACHE_DIR = Path.home() / ".local" / "todu" / "forgejo"
ITEMS_DIR = Path.home() / ".local" / "todu" / "issues"

def get_forgejo_url(cwd=None):
    """Get Forgejo base URL from git remote in cwd."""
    # Try to extract from git remote in the current directory
    try:
        result = subprocess.run(
            ['git', 'remote', 'get-url', 'origin'],
            capture_output=True,
            text=True,
            check=True,
            cwd=cwd
        )
        remote_url = result.stdout.strip()

        # Parse URL to extract base domain
        # Handle both SSH and HTTPS URLs
        if remote_url.startswith('ssh://git@'):
            # SSH format: ssh://git@forgejo.example.com/owner/repo.git
            host = remote_url.replace('ssh://git@', '').split('/')[0]
            base_url = f"https://{host}"
        elif remote_url.startswith('git@'):
            # SSH format: git@forgejo.example.com:owner/repo.git
            host = remote_url.split('@')[1].split(':')[0]
            base_url = f"https://{host}"
        elif remote_url.startswith('http'):
            # HTTPS format: https://forgejo.example.com/owner/repo.git
            from urllib.parse import urlparse
            parsed = urlparse(remote_url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
        else:
            base_url = None

        # Reject github.com
        if base_url and 'github.com' in base_url:
            print(json.dumps({
                "error": "This appears to be a GitHub repository, not Forgejo",
                "help": "Use the github plugin for GitHub repositories"
            }), file=sys.stderr)
            sys.exit(1)

        if base_url:
            return base_url
    except Exception:
        pass

    print(json.dumps({
        "error": "Could not detect Forgejo URL from git remote",
        "help": "Make sure you are in a git repository with a Forgejo remote"
    }), file=sys.stderr)
    sys.exit(1)

def normalize_issue(issue, repo_name):
    """Convert Forgejo issue to normalized format."""
    # Extract status from status:* label, fallback to state
    labels = [label['name'] for label in issue.get('labels', [])]
    status = None
    for label in labels:
        if label.startswith('status:'):
            status = label.split(':', 1)[1]
            break

    # If no status label, derive from Forgejo state
    if not status:
        status = "open" if issue['state'] == "open" else "closed"

    # Add completedAt timestamp for completed issues (NOT canceled)
    completed_at = None
    if status in ["closed", "done"]:
        # Use closed_at if available, otherwise fall back to updated_at
        if issue.get('closed_at'):
            completed_at = issue['closed_at']
        else:
            completed_at = issue['updated_at']

    # Extract standardized priority from labels
    priority = None
    for label in labels:
        if label.startswith("priority:"):
            priority = label.split(":", 1)[1]
            break

    normalized = {
        "id": None,  # Will be assigned below
        "system": "forgejo",
        "type": "issue",
        "title": issue['title'],
        "description": issue['body'] or "",
        "state": issue['state'],  # System-level state: "open" or "closed"
        "status": status,  # Workflow-level status from labels
        "url": issue['html_url'],
        "createdAt": issue['created_at'],
        "updatedAt": issue['updated_at'],
        "labels": labels,
        "assignees": [assignee['login'] for assignee in (issue.get('assignees') or [])],
        "priority": priority,  # Standardized priority field
        "dueDate": None,  # Forgejo issues don't have due dates
        "systemData": {
            "repo": repo_name,
            "number": issue['number'],
            "state": issue['state'],
            "state_reason": issue.get('state_reason', None)
        }
    }

    # Only include completedAt if the issue is completed
    if completed_at:
        normalized["completedAt"] = completed_at

    return normalized

def sync_issues(repo_name, since=None, issue_number=None, base_url=None):
    """Sync Forgejo issues to local cache."""
    token = os.environ.get('FORGEJO_TOKEN')
    if not token:
        print(json.dumps({"error": "FORGEJO_TOKEN environment variable not set"}), file=sys.stderr)
        sys.exit(1)

    if not base_url:
        base_url = get_forgejo_url()

    try:
        headers = {
            'Authorization': f'token {token}',
            'Content-Type': 'application/json'
        }

        # Create cache directory
        ITEMS_DIR.mkdir(parents=True, exist_ok=True)

        # Use repo name prefix in filename to avoid conflicts
        # Replace '/' with '_' for valid filename
        repo_prefix = repo_name.replace('/', '_')

        # Fetch issues based on mode
        if issue_number:
            # Single issue mode
            api_url = f"{base_url}/api/v1/repos/{repo_name}/issues/{issue_number}"
            response = requests.get(api_url, headers=headers)
            response.raise_for_status()
            issue = response.json()

            # Check if it's a pull request
            if issue.get('pull_request'):
                print(json.dumps({"error": f"Issue #{issue_number} is a pull request, not an issue"}), file=sys.stderr)
                return 1

            issues = [issue]
            sync_mode = "single"
        else:
            # Full or incremental sync mode
            api_url = f"{base_url}/api/v1/repos/{repo_name}/issues"
            params = {
                'state': 'all',
                'page': 1,
                'limit': 100
            }

            if since:
                params['since'] = since.isoformat()
                sync_mode = "incremental"
            else:
                sync_mode = "full"

            issues = []
            while True:
                response = requests.get(api_url, headers=headers, params=params)
                response.raise_for_status()
                page_issues = response.json()

                if not page_issues:
                    break

                # Filter out pull requests
                issues.extend([i for i in page_issues if not i.get('pull_request')])

                params['page'] += 1

        new_count = 0
        updated_count = 0

        for issue in issues:
            # Use system and repo prefix in filename to avoid conflicts
            filename = f"forgejo-{repo_prefix}-{issue['number']}.json"
            issue_file = ITEMS_DIR / filename
            is_new = not issue_file.exists()

            # Save normalized issue
            normalized = normalize_issue(issue, repo_name)

            # Assign or reuse todu ID
            if is_new:
                # New issue: assign new todu ID
                todu_id = assign_id(filename)
            else:
                # Existing issue: look up existing todu ID
                todu_id = lookup_id(filename)
                if todu_id is None:
                    # File exists but not in registry (shouldn't happen, but handle it)
                    todu_id = assign_id(filename)

            normalized["id"] = todu_id

            with open(issue_file, 'w') as f:
                json.dump(normalized, f, indent=2)

            if is_new:
                new_count += 1
            else:
                updated_count += 1

        # Update sync metadata in unified file
        update_sync_metadata(
            system="forgejo",
            mode=sync_mode,
            task_count=new_count + updated_count,
            repo=repo_name
        )

        result = {
            "synced": new_count + updated_count,
            "new": new_count,
            "updated": updated_count,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "mode": sync_mode
        }

        # Add issue number for single issue sync
        if issue_number:
            result["issue"] = f"#{issue_number}"

        print(json.dumps(result, indent=2))
        return 0

    except requests.exceptions.RequestException as e:
        import traceback
        error_msg = str(e)
        if hasattr(e, 'response') and e.response is not None:
            error_msg = f"{error_msg}: {e.response.text}"
        error_info = {
            "error": error_msg,
            "type": type(e).__name__,
            "traceback": traceback.format_exc()
        }
        print(json.dumps(error_info), file=sys.stderr)
        return 1
    except Exception as e:
        import traceback
        error_info = {
            "error": str(e),
            "type": type(e).__name__,
            "traceback": traceback.format_exc()
        }
        print(json.dumps(error_info), file=sys.stderr)
        return 1

def main():
    parser = argparse.ArgumentParser(description='Sync Forgejo issues to local cache')
    parser.add_argument('--repo', required=True, help='Repository in owner/name format')
    parser.add_argument('--since', help='ISO timestamp to fetch issues since')
    parser.add_argument('--issue', type=int, help='Sync specific issue number')
    parser.add_argument('--base-url', help='Forgejo base URL (e.g., https://forgejo.example.com)')

    args = parser.parse_args()

    # Make --issue and --since mutually exclusive
    if args.issue and args.since:
        parser.error("Cannot specify both --issue and --since")

    since = datetime.fromisoformat(args.since.replace('Z', '+00:00')) if args.since else None

    return sync_issues(args.repo, since, args.issue, args.base_url)

if __name__ == '__main__':
    sys.exit(main())
